{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anil-mannem/INFO-5731-Spring-2024/blob/main/Mannem_Anil_Exercise_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU-pLW33lpcS"
      },
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loi8Sh7UE6ha"
      },
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Saving the train data in a dataframe\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\n",
        "train_data['sentiment'] = train_data['reviews'].str[0].astype(int)\n",
        "train_data['reviews'] = train_data['reviews'].str[2:]\n",
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "5IQMmjcfsRQJ",
        "outputId": "1f32587d-9ce1-4b61-a87c-409e1e4f4e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-eca3dfff984d>:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  train_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             reviews  sentiment\n",
              "0  a stirring , funny and finally transporting re...          1\n",
              "1  apparently reassembled from the cutting-room f...          0\n",
              "2  they presume their audience wo n't sit still f...          0\n",
              "3  this is a visually stunning rumination on love...          1\n",
              "4  jonathan parker 's bartleby should have been t...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1142aae2-1c1c-40a9-876f-9c6d45717da7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting-room f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1142aae2-1c1c-40a9-876f-9c6d45717da7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1142aae2-1c1c-40a9-876f-9c6d45717da7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1142aae2-1c1c-40a9-876f-9c6d45717da7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8a5e99bf-fdd6-4cf1-a1c0-a9f929d0dba5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8a5e99bf-fdd6-4cf1-a1c0-a9f929d0dba5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8a5e99bf-fdd6-4cf1-a1c0-a9f929d0dba5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data",
              "summary": "{\n  \"name\": \"train_data\",\n  \"rows\": 6920,\n  \"fields\": [\n    {\n      \"column\": \"reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6911,\n        \"samples\": [\n          \"a loud , brash and mainly unfunny high school comedy .\",\n          \"the real star of this movie is the score , as in the songs translate well to film , and it 's really well directed .\",\n          \"rosenthal -lrb- halloween ii -rrb- seems to have forgotten everything he ever knew about generating suspense .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write your code here\n",
        "import pandas as pd\n",
        "\n",
        "test_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\n",
        "test_data['sentiment'] = test_data['reviews'].str[0].astype(int)\n",
        "test_data['reviews'] = test_data['reviews'].str[2:]\n",
        "test_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "p_oc-mSmtBCL",
        "outputId": "15603e26-58c5-4d9b-8cd8-30a4d7c53a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-4fb57f57bc08>:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  test_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             reviews  sentiment\n",
              "0     no movement , no yuks , not much of anything .          0\n",
              "1  a gob of drivel so sickly sweet , even the eag...          0\n",
              "2  gangs of new york is an unapologetic mess , wh...          0\n",
              "3  we never really feel involved with the story ,...          0\n",
              "4            this is one of polanski 's best films .          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-279e7fdc-e17f-4e23-a90e-833b3bef92ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no movement , no yuks , not much of anything .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we never really feel involved with the story ,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is one of polanski 's best films .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-279e7fdc-e17f-4e23-a90e-833b3bef92ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-279e7fdc-e17f-4e23-a90e-833b3bef92ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-279e7fdc-e17f-4e23-a90e-833b3bef92ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e8e867c-518c-460d-9df0-4d38fa737fa3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e8e867c-518c-460d-9df0-4d38fa737fa3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e8e867c-518c-460d-9df0-4d38fa737fa3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data",
              "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 1821,\n  \"fields\": [\n    {\n      \"column\": \"reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1821,\n        \"samples\": [\n          \"there is no entry portal in the rules of attraction , and i spent most of the movie feeling depressed by the shallow , selfish , greedy characters .\",\n          \"a charming yet poignant tale of the irrevocable ties that bind .\",\n          \"it 's all pretty cynical and condescending , too .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()\n",
        "\n",
        "def clean(reviews):\n",
        "    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\n",
        "    review = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", reviews)\n",
        "    tokens = re.split('\\W+',reviews)\n",
        "    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "    return review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hxxTC00tB8X",
        "outputId": "0335fc48-5481-493c-f7c7-6636dea86885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\n",
        "train_data['sentiment'] = train_data['reviews'].str[0].astype(int)\n",
        "train_data['reviews'] = train_data['reviews'].str[2:]\n",
        "\n",
        "test_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\n",
        "test_data['sentiment'] = test_data['reviews'].str[0].astype(int)\n",
        "test_data['reviews'] = test_data['reviews'].str[2:]\n",
        "\n",
        "# Define custom text preprocessing function\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
        "    tokens = re.split(r'\\W+', text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Create TF-IDF vectorizer with custom preprocessing\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', preprocessor=preprocess_text, stop_words='english', max_features=13343)\n",
        "\n",
        "# Fit and transform data\n",
        "X_tfidf = tfidf_vect.fit_transform(train_data['reviews'])\n",
        "X_test_tfidf = tfidf_vect.transform(test_data['reviews'])\n",
        "\n",
        "print(f\"Training data shape: {X_tfidf.shape}\")\n",
        "print(f\"Test data shape: {X_test_tfidf.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhuSArGKtF5T",
        "outputId": "8887d764-ba4b-4537-ec2e-477226c48ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-cdb6592617bb>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  train_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\n",
            "<ipython-input-28-cdb6592617bb>:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  test_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (6920, 13081)\n",
            "Test data shape: (1821, 13081)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf_df = vectorizer.fit_transform(train_data['reviews'].values)\n",
        "print(globals())\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "# Sampling the training set\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\n",
        "                                                test_size=0.2, random_state=42)\n",
        "# Model fit using .fit()\n",
        "model_mnb = mnb.fit(x_train,y_train)\n",
        "y_pred_mnb = model_mnb.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\n",
        "print(classification_report(y_test,y_pred_mnb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qus7uJ19tHAI",
        "outputId": "29ff8930-3148-46b1-f0ce-11876a408c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"get_ipython().run_line_magic('pip', 'install sentence-transformers')\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Remove rows with NaN values in the 'Reviews' column\\ndata = data[data['Reviews'].notnull()]\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\\n\\n# Perform clustering or other downstream tasks using X\", '# K-means clustering\\nprint(\"Performing K-means clustering...\")\\nkmeans = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\\nkmeans.fit(X)\\nprint(\"K-means clustering complete.\")\\n\\n# Plot K-means clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans.labels_)\\nplt.title(\"K-means Clustering\")\\nplt.show()', '# DBSCAN clustering\\nprint(\"Performing DBSCAN clustering...\")\\ndbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed\\ndbscan.fit(X)\\nprint(\"DBSCAN clustering complete.\")\\n\\n# Plot DBSCAN clusters\\n#tsne = TSNE(n_components=2, random_state=42)\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan.labels_)\\nplt.title(\"DBSCAN Clustering\")\\nplt.show()', '# Hierarchical clustering\\nprint(\"Performing hierarchical clustering...\")\\nhierarchical = DBSCAN(eps=0.5, min_samples=5)# Adjust the number of clusters as needed\\nhierarchical.fit(X)\\nprint(\"Hierarchical clustering complete.\")\\n\\n# Plot hierarchical clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical.labels_)\\nplt.title(\"Hierarchical Clustering\")\\nplt.show()', '# Hierarchical clustering\\nprint(\"Performing hierarchical clustering...\")\\nhierarchical = DBSCAN(eps=0.5, min_samples=5)# Adjust the number of clusters as needed\\nhierarchical.fit(X)\\nprint(\"Hierarchical clustering complete.\")\\n\\n# Plot hierarchical clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical.labels_)\\nplt.title(\"Hierarchical Clustering\")\\nplt.show()', \"# Write your code here\\n# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", '# DBSCAN clustering\\nprint(\"Performing DBSCAN clustering...\")\\nmy_dbscan = SKDBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed\\nmy_dbscan.fit(my_X)\\nprint(\"DBSCAN clustering complete.\")\\n\\n# Plot DBSCAN clusters\\nmy_tsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nmy_X_tsne = my_tsne.fit_transform(my_X)\\nplt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_dbscan.labels_)\\nplt.title(\"DBSCAN Clustering\")', \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN as SKDBSCAN\\n\\n# Install the sentence_transformers module if it is not already installed\\nget_ipython().system('pip install sentence-transformers')\\n\\n# Load the dataset\\nmy_data = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Remove rows with NaN values in the 'Reviews' column\\nmy_data = my_data[my_data['Reviews'].notnull()]\\n\\n# Create a TF-IDF vectorizer\\nmy_vectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nmy_X = my_vectorizer.fit_transform(my_data['Reviews'])\\n\\n# Perform clustering or other downstream tasks using my_X\", '# K-means clustering\\nprint(\"Performing K-means clustering...\")\\nmy_kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\\nmy_kmeans.fit(my_X)\\nprint(\"K-means clustering complete.\")\\n\\n# Plot K-means clusters\\nmy_tsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nmy_X_tsne = my_tsne.fit_transform(my_X)\\nplt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_kmeans.labels_)\\nplt.title(\"K-means Clustering\")\\nplt.show()', \"# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", 'import nltk\\nimport re\\nimport string\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopword=nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nwl= WordNetLemmatizer()\\n\\ndef clean(reviews):\\n    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\\n    review = re.sub(\"^\\\\d+\\\\s|\\\\s\\\\d+\\\\s|\\\\s\\\\d+$\", \" \", reviews)\\n    tokens = re.split(\\'\\\\W+\\',reviews)\\n    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\\n    return review', 'import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Load and preprocess data\\ntrain_data = pd.read_csv(\\'stsa-train.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntrain_data[\\'sentiment\\'] = train_data[\\'reviews\\'].str[0].astype(int)\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str[2:]\\n\\ntest_data = pd.read_csv(\\'stsa-test.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntest_data[\\'sentiment\\'] = test_data[\\'reviews\\'].str[0].astype(int)\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str[2:]\\n\\n# Define custom text preprocessing function\\nimport nltk\\nimport re\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopwords = nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    text = re.sub(r\\'[^a-zA-Z\\\\s]\\', \\'\\', text)  # Remove non-alphabetic characters\\n    tokens = re.split(r\\'\\\\W+\\', text.lower())\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\\n    return \\' \\'.join(tokens)\\n\\n# Create TF-IDF vectorizer with custom preprocessing\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', preprocessor=preprocess_text, stop_words=\\'english\\', max_features=13343)\\n\\n# Fit and transform data\\nX_tfidf = tfidf_vect.fit_transform(train_data[\\'reviews\\'])\\nX_test_tfidf = tfidf_vect.transform(test_data[\\'reviews\\'])\\n\\nprint(f\"Training data shape: {X_tfidf.shape}\")\\nprint(f\"Test data shape: {X_test_tfidf.shape}\")', \"import pandas as pd\\nfrom sklearn import datasets, linear_model\\nfrom sklearn.model_selection import train_test_split\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.naive_bayes import MultinomialNB\\nvectorizer = TfidfVectorizer()\\nX_tfidf_df = vectorizer.fit_transform(train_data['reviews'].values)\\nprint(globals())\\nmnb = MultinomialNB()\\n\\n# Sampling the training set\\nx_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\\n                                                test_size=0.2, random_state=42)\\n# Model fit using .fit()\\nmodel_mnb = mnb.fit(x_train,y_train)\\ny_pred_mnb = model_mnb.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\\nprint(classification_report(y_test,y_pred_mnb))\", 'from sklearn.model_selection import cross_val_score\\nscores = cross_val_score(mnb, x_test, y_test, cv=10)\\nprint(\"MultinominalNB score: \",scores.mean())', \"from sklearn.svm import LinearSVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\\nfrom xgboost import XGBClassifier\\n\\nsvm = LinearSVC()\\nmodel_svm = svm.fit(x_train,y_train)\\ny_pred_svm = model_svm.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_svm,y_test))\\nprint(classification_report(y_test,y_pred_svm))\", 'from sklearn.model_selection import cross_val_score\\nscores = cross_val_score(svm, x_test, y_test, cv=10)\\nprint(\"SVM score:\",scores.mean())', \"# 3) KNeighbor\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\nknn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\\nmodel_knn = knn.fit(x_train,y_train)\\ny_pred_knn = model_knn.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_knn,y_test))\\nprint(classification_report(y_test,y_pred_knn))\", \"# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", 'import nltk\\nimport re\\nimport string\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopword=nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nwl= WordNetLemmatizer()\\n\\ndef clean(reviews):\\n    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\\n    review = re.sub(\"^\\\\d+\\\\s|\\\\s\\\\d+\\\\s|\\\\s\\\\d+$\", \" \", reviews)\\n    tokens = re.split(\\'\\\\W+\\',reviews)\\n    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\\n    return review', 'import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Load and preprocess data\\ntrain_data = pd.read_csv(\\'stsa-train.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntrain_data[\\'sentiment\\'] = train_data[\\'reviews\\'].str[0].astype(int)\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str[2:]\\n\\ntest_data = pd.read_csv(\\'stsa-test.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntest_data[\\'sentiment\\'] = test_data[\\'reviews\\'].str[0].astype(int)\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str[2:]\\n\\n# Define custom text preprocessing function\\nimport nltk\\nimport re\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopwords = nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    text = re.sub(r\\'[^a-zA-Z\\\\s]\\', \\'\\', text)  # Remove non-alphabetic characters\\n    tokens = re.split(r\\'\\\\W+\\', text.lower())\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\\n    return \\' \\'.join(tokens)\\n\\n# Create TF-IDF vectorizer with custom preprocessing\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', preprocessor=preprocess_text, stop_words=\\'english\\', max_features=13343)\\n\\n# Fit and transform data\\nX_tfidf = tfidf_vect.fit_transform(train_data[\\'reviews\\'])\\nX_test_tfidf = tfidf_vect.transform(test_data[\\'reviews\\'])\\n\\nprint(f\"Training data shape: {X_tfidf.shape}\")\\nprint(f\"Test data shape: {X_test_tfidf.shape}\")', \"import pandas as pd\\nfrom sklearn import datasets, linear_model\\nfrom sklearn.model_selection import train_test_split\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.naive_bayes import MultinomialNB\\nvectorizer = TfidfVectorizer()\\nX_tfidf_df = vectorizer.fit_transform(train_data['reviews'].values)\\nprint(globals())\\nmnb = MultinomialNB()\\n\\n# Sampling the training set\\nx_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\\n                                                test_size=0.2, random_state=42)\\n# Model fit using .fit()\\nmodel_mnb = mnb.fit(x_train,y_train)\\ny_pred_mnb = model_mnb.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\\nprint(classification_report(y_test,y_pred_mnb))\"], '_oh': {16:                                              reviews  sentiment\n",
            "0  a stirring , funny and finally transporting re...          1\n",
            "1  apparently reassembled from the cutting-room f...          0\n",
            "2  they presume their audience wo n't sit still f...          0\n",
            "3  this is a visually stunning rumination on love...          1\n",
            "4  jonathan parker 's bartleby should have been t...          1, 17:                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1, 25:                                              reviews  sentiment\n",
            "0  a stirring , funny and finally transporting re...          1\n",
            "1  apparently reassembled from the cutting-room f...          0\n",
            "2  they presume their audience wo n't sit still f...          0\n",
            "3  this is a visually stunning rumination on love...          1\n",
            "4  jonathan parker 's bartleby should have been t...          1, 26:                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1}, '_dh': ['/content'], 'In': ['', \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"get_ipython().run_line_magic('pip', 'install sentence-transformers')\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Remove rows with NaN values in the 'Reviews' column\\ndata = data[data['Reviews'].notnull()]\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\\n\\n# Perform clustering or other downstream tasks using X\", '# K-means clustering\\nprint(\"Performing K-means clustering...\")\\nkmeans = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\\nkmeans.fit(X)\\nprint(\"K-means clustering complete.\")\\n\\n# Plot K-means clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans.labels_)\\nplt.title(\"K-means Clustering\")\\nplt.show()', '# DBSCAN clustering\\nprint(\"Performing DBSCAN clustering...\")\\ndbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed\\ndbscan.fit(X)\\nprint(\"DBSCAN clustering complete.\")\\n\\n# Plot DBSCAN clusters\\n#tsne = TSNE(n_components=2, random_state=42)\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan.labels_)\\nplt.title(\"DBSCAN Clustering\")\\nplt.show()', '# Hierarchical clustering\\nprint(\"Performing hierarchical clustering...\")\\nhierarchical = DBSCAN(eps=0.5, min_samples=5)# Adjust the number of clusters as needed\\nhierarchical.fit(X)\\nprint(\"Hierarchical clustering complete.\")\\n\\n# Plot hierarchical clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical.labels_)\\nplt.title(\"Hierarchical Clustering\")\\nplt.show()', '# Hierarchical clustering\\nprint(\"Performing hierarchical clustering...\")\\nhierarchical = DBSCAN(eps=0.5, min_samples=5)# Adjust the number of clusters as needed\\nhierarchical.fit(X)\\nprint(\"Hierarchical clustering complete.\")\\n\\n# Plot hierarchical clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical.labels_)\\nplt.title(\"Hierarchical Clustering\")\\nplt.show()', \"# Write your code here\\n# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", '# DBSCAN clustering\\nprint(\"Performing DBSCAN clustering...\")\\nmy_dbscan = SKDBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed\\nmy_dbscan.fit(my_X)\\nprint(\"DBSCAN clustering complete.\")\\n\\n# Plot DBSCAN clusters\\nmy_tsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nmy_X_tsne = my_tsne.fit_transform(my_X)\\nplt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_dbscan.labels_)\\nplt.title(\"DBSCAN Clustering\")', \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN as SKDBSCAN\\n\\n# Install the sentence_transformers module if it is not already installed\\nget_ipython().system('pip install sentence-transformers')\\n\\n# Load the dataset\\nmy_data = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Remove rows with NaN values in the 'Reviews' column\\nmy_data = my_data[my_data['Reviews'].notnull()]\\n\\n# Create a TF-IDF vectorizer\\nmy_vectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nmy_X = my_vectorizer.fit_transform(my_data['Reviews'])\\n\\n# Perform clustering or other downstream tasks using my_X\", '# K-means clustering\\nprint(\"Performing K-means clustering...\")\\nmy_kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\\nmy_kmeans.fit(my_X)\\nprint(\"K-means clustering complete.\")\\n\\n# Plot K-means clusters\\nmy_tsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nmy_X_tsne = my_tsne.fit_transform(my_X)\\nplt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_kmeans.labels_)\\nplt.title(\"K-means Clustering\")\\nplt.show()', \"# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", 'import nltk\\nimport re\\nimport string\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopword=nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nwl= WordNetLemmatizer()\\n\\ndef clean(reviews):\\n    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\\n    review = re.sub(\"^\\\\d+\\\\s|\\\\s\\\\d+\\\\s|\\\\s\\\\d+$\", \" \", reviews)\\n    tokens = re.split(\\'\\\\W+\\',reviews)\\n    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\\n    return review', 'import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Load and preprocess data\\ntrain_data = pd.read_csv(\\'stsa-train.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntrain_data[\\'sentiment\\'] = train_data[\\'reviews\\'].str[0].astype(int)\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str[2:]\\n\\ntest_data = pd.read_csv(\\'stsa-test.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntest_data[\\'sentiment\\'] = test_data[\\'reviews\\'].str[0].astype(int)\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str[2:]\\n\\n# Define custom text preprocessing function\\nimport nltk\\nimport re\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopwords = nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    text = re.sub(r\\'[^a-zA-Z\\\\s]\\', \\'\\', text)  # Remove non-alphabetic characters\\n    tokens = re.split(r\\'\\\\W+\\', text.lower())\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\\n    return \\' \\'.join(tokens)\\n\\n# Create TF-IDF vectorizer with custom preprocessing\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', preprocessor=preprocess_text, stop_words=\\'english\\', max_features=13343)\\n\\n# Fit and transform data\\nX_tfidf = tfidf_vect.fit_transform(train_data[\\'reviews\\'])\\nX_test_tfidf = tfidf_vect.transform(test_data[\\'reviews\\'])\\n\\nprint(f\"Training data shape: {X_tfidf.shape}\")\\nprint(f\"Test data shape: {X_test_tfidf.shape}\")', \"import pandas as pd\\nfrom sklearn import datasets, linear_model\\nfrom sklearn.model_selection import train_test_split\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.naive_bayes import MultinomialNB\\nvectorizer = TfidfVectorizer()\\nX_tfidf_df = vectorizer.fit_transform(train_data['reviews'].values)\\nprint(globals())\\nmnb = MultinomialNB()\\n\\n# Sampling the training set\\nx_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\\n                                                test_size=0.2, random_state=42)\\n# Model fit using .fit()\\nmodel_mnb = mnb.fit(x_train,y_train)\\ny_pred_mnb = model_mnb.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\\nprint(classification_report(y_test,y_pred_mnb))\", 'from sklearn.model_selection import cross_val_score\\nscores = cross_val_score(mnb, x_test, y_test, cv=10)\\nprint(\"MultinominalNB score: \",scores.mean())', \"from sklearn.svm import LinearSVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\\nfrom xgboost import XGBClassifier\\n\\nsvm = LinearSVC()\\nmodel_svm = svm.fit(x_train,y_train)\\ny_pred_svm = model_svm.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_svm,y_test))\\nprint(classification_report(y_test,y_pred_svm))\", 'from sklearn.model_selection import cross_val_score\\nscores = cross_val_score(svm, x_test, y_test, cv=10)\\nprint(\"SVM score:\",scores.mean())', \"# 3) KNeighbor\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\nknn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\\nmodel_knn = knn.fit(x_train,y_train)\\ny_pred_knn = model_knn.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_knn,y_test))\\nprint(classification_report(y_test,y_pred_knn))\", \"# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", 'import nltk\\nimport re\\nimport string\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopword=nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nwl= WordNetLemmatizer()\\n\\ndef clean(reviews):\\n    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\\n    review = re.sub(\"^\\\\d+\\\\s|\\\\s\\\\d+\\\\s|\\\\s\\\\d+$\", \" \", reviews)\\n    tokens = re.split(\\'\\\\W+\\',reviews)\\n    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\\n    return review', 'import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Load and preprocess data\\ntrain_data = pd.read_csv(\\'stsa-train.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntrain_data[\\'sentiment\\'] = train_data[\\'reviews\\'].str[0].astype(int)\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str[2:]\\n\\ntest_data = pd.read_csv(\\'stsa-test.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntest_data[\\'sentiment\\'] = test_data[\\'reviews\\'].str[0].astype(int)\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str[2:]\\n\\n# Define custom text preprocessing function\\nimport nltk\\nimport re\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopwords = nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    text = re.sub(r\\'[^a-zA-Z\\\\s]\\', \\'\\', text)  # Remove non-alphabetic characters\\n    tokens = re.split(r\\'\\\\W+\\', text.lower())\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\\n    return \\' \\'.join(tokens)\\n\\n# Create TF-IDF vectorizer with custom preprocessing\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', preprocessor=preprocess_text, stop_words=\\'english\\', max_features=13343)\\n\\n# Fit and transform data\\nX_tfidf = tfidf_vect.fit_transform(train_data[\\'reviews\\'])\\nX_test_tfidf = tfidf_vect.transform(test_data[\\'reviews\\'])\\n\\nprint(f\"Training data shape: {X_tfidf.shape}\")\\nprint(f\"Test data shape: {X_test_tfidf.shape}\")', \"import pandas as pd\\nfrom sklearn import datasets, linear_model\\nfrom sklearn.model_selection import train_test_split\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.naive_bayes import MultinomialNB\\nvectorizer = TfidfVectorizer()\\nX_tfidf_df = vectorizer.fit_transform(train_data['reviews'].values)\\nprint(globals())\\nmnb = MultinomialNB()\\n\\n# Sampling the training set\\nx_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\\n                                                test_size=0.2, random_state=42)\\n# Model fit using .fit()\\nmodel_mnb = mnb.fit(x_train,y_train)\\ny_pred_mnb = model_mnb.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\\nprint(classification_report(y_test,y_pred_mnb))\"], 'Out': {16:                                              reviews  sentiment\n",
            "0  a stirring , funny and finally transporting re...          1\n",
            "1  apparently reassembled from the cutting-room f...          0\n",
            "2  they presume their audience wo n't sit still f...          0\n",
            "3  this is a visually stunning rumination on love...          1\n",
            "4  jonathan parker 's bartleby should have been t...          1, 17:                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1, 25:                                              reviews  sentiment\n",
            "0  a stirring , funny and finally transporting re...          1\n",
            "1  apparently reassembled from the cutting-room f...          0\n",
            "2  they presume their audience wo n't sit still f...          0\n",
            "3  this is a visually stunning rumination on love...          1\n",
            "4  jonathan parker 's bartleby should have been t...          1, 26:                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1}, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x79591c07f820>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x79591c07ece0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x79591c07ece0>, '_':                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1, '__':                                              reviews  sentiment\n",
            "0  a stirring , funny and finally transporting re...          1\n",
            "1  apparently reassembled from the cutting-room f...          0\n",
            "2  they presume their audience wo n't sit still f...          0\n",
            "3  this is a visually stunning rumination on love...          1\n",
            "4  jonathan parker 's bartleby should have been t...          1, '___':                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1, '_i': 'import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Load and preprocess data\\ntrain_data = pd.read_csv(\\'stsa-train.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntrain_data[\\'sentiment\\'] = train_data[\\'reviews\\'].str[0].astype(int)\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str[2:]\\n\\ntest_data = pd.read_csv(\\'stsa-test.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntest_data[\\'sentiment\\'] = test_data[\\'reviews\\'].str[0].astype(int)\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str[2:]\\n\\n# Define custom text preprocessing function\\nimport nltk\\nimport re\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopwords = nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    text = re.sub(r\\'[^a-zA-Z\\\\s]\\', \\'\\', text)  # Remove non-alphabetic characters\\n    tokens = re.split(r\\'\\\\W+\\', text.lower())\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\\n    return \\' \\'.join(tokens)\\n\\n# Create TF-IDF vectorizer with custom preprocessing\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', preprocessor=preprocess_text, stop_words=\\'english\\', max_features=13343)\\n\\n# Fit and transform data\\nX_tfidf = tfidf_vect.fit_transform(train_data[\\'reviews\\'])\\nX_test_tfidf = tfidf_vect.transform(test_data[\\'reviews\\'])\\n\\nprint(f\"Training data shape: {X_tfidf.shape}\")\\nprint(f\"Test data shape: {X_test_tfidf.shape}\")', '_ii': 'import nltk\\nimport re\\nimport string\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopword=nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nwl= WordNetLemmatizer()\\n\\ndef clean(reviews):\\n    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\\n    review = re.sub(\"^\\\\d+\\\\s|\\\\s\\\\d+\\\\s|\\\\s\\\\d+$\", \" \", reviews)\\n    tokens = re.split(\\'\\\\W+\\',reviews)\\n    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\\n    return review', '_iii': \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", '_i1': \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", 'pd': <module 'pandas' from '/usr/local/lib/python3.10/dist-packages/pandas/__init__.py'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'KMeans': <class 'sklearn.cluster._kmeans.KMeans'>, 'DBSCAN': <class 'sklearn.cluster._dbscan.DBSCAN'>, 'AgglomerativeClustering': <class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>, 'gensim': <module 'gensim' from '/usr/local/lib/python3.10/dist-packages/gensim/__init__.py'>, '_i2': \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", '_i3': \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", '_i4': 'pip install sentence-transformers', '_exit_code': 0, '_i5': \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Preprocess the text data (if needed)\\n# For example, remove HTML tags, handle missing values, etc.\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\", 'SentenceTransformer': <class 'sentence_transformers.SentenceTransformer.SentenceTransformer'>, 'plt': <module 'matplotlib.pyplot' from '/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py'>, 'TSNE': <class 'sklearn.manifold._t_sne.TSNE'>, 'data':                                             Product Name Brand Name   Price  \\\n",
            "0      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "...                                                  ...        ...     ...   \n",
            "12381  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12382  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12383  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12384  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12385  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "\n",
            "       Rating                                            Reviews  Review Votes  \n",
            "0         5.0  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1         4.0  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2         5.0                                       Very pleased           0.0  \n",
            "3         4.0  It works good but it goes slow sometimes but i...           0.0  \n",
            "4         4.0  Great phone to replace my lost phone. The only...           0.0  \n",
            "...       ...                                                ...           ...  \n",
            "12381     4.0                                        Worked fine           0.0  \n",
            "12382     5.0  This phone works well. It replaced a damaged u...           0.0  \n",
            "12383     5.0  I am very happy with my iPhone 4s but you must...           0.0  \n",
            "12384     5.0                                            Perfect           0.0  \n",
            "12385     5.0                                      Perfect item.           0.0  \n",
            "\n",
            "[12385 rows x 6 columns], 'vectorizer': TfidfVectorizer(), '_i6': \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN\\n\\n# Load the dataset\\ndata = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Remove rows with NaN values in the 'Reviews' column\\ndata = data[data['Reviews'].notnull()]\\n\\n# Create a TF-IDF vectorizer\\nvectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nX = vectorizer.fit_transform(data['Reviews'])\\n\\n# Perform clustering or other downstream tasks using X\", 'X': <12385x11419 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 344866 stored elements in Compressed Sparse Row format>, '_i7': '# K-means clustering\\nprint(\"Performing K-means clustering...\")\\nkmeans = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\\nkmeans.fit(X)\\nprint(\"K-means clustering complete.\")\\n\\n# Plot K-means clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans.labels_)\\nplt.title(\"K-means Clustering\")\\nplt.show()', 'kmeans': KMeans(n_clusters=5, random_state=42), 'tsne': TSNE(init='random', random_state=42), 'X_tsne': array([[ -9.533273 , 117.64673  ],\n",
            "       [-22.019907 , 115.2638   ],\n",
            "       [ -1.0859123,  42.85316  ],\n",
            "       ...,\n",
            "       [ 60.858944 , -96.39294  ],\n",
            "       [-58.955406 ,  27.47482  ],\n",
            "       [-54.779903 ,   7.3074155]], dtype=float32), '_i8': '# DBSCAN clustering\\nprint(\"Performing DBSCAN clustering...\")\\ndbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed\\ndbscan.fit(X)\\nprint(\"DBSCAN clustering complete.\")\\n\\n# Plot DBSCAN clusters\\n#tsne = TSNE(n_components=2, random_state=42)\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan.labels_)\\nplt.title(\"DBSCAN Clustering\")\\nplt.show()', 'dbscan': DBSCAN(), '_i9': '# Hierarchical clustering\\nprint(\"Performing hierarchical clustering...\")\\nhierarchical = DBSCAN(eps=0.5, min_samples=5)# Adjust the number of clusters as needed\\nhierarchical.fit(X)\\nprint(\"Hierarchical clustering complete.\")\\n\\n# Plot hierarchical clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical.labels_)\\nplt.title(\"Hierarchical Clustering\")\\nplt.show()', 'hierarchical': DBSCAN(), '_i10': '# Hierarchical clustering\\nprint(\"Performing hierarchical clustering...\")\\nhierarchical = DBSCAN(eps=0.5, min_samples=5)# Adjust the number of clusters as needed\\nhierarchical.fit(X)\\nprint(\"Hierarchical clustering complete.\")\\n\\n# Plot hierarchical clusters\\ntsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nX_tsne = tsne.fit_transform(X)\\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical.labels_)\\nplt.title(\"Hierarchical Clustering\")\\nplt.show()', '_i11': \"# Write your code here\\n# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", '_i12': \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", '_i13': '# DBSCAN clustering\\nprint(\"Performing DBSCAN clustering...\")\\nmy_dbscan = SKDBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed\\nmy_dbscan.fit(my_X)\\nprint(\"DBSCAN clustering complete.\")\\n\\n# Plot DBSCAN clusters\\nmy_tsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nmy_X_tsne = my_tsne.fit_transform(my_X)\\nplt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_dbscan.labels_)\\nplt.title(\"DBSCAN Clustering\")', '_i14': \"import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\\nimport gensim\\nfrom sentence_transformers import SentenceTransformer\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.cluster import DBSCAN as SKDBSCAN\\n\\n# Install the sentence_transformers module if it is not already installed\\n!pip install sentence-transformers\\n\\n# Load the dataset\\nmy_data = pd.read_csv('Amazon_Unlocked_Mobile.csv')\\n\\n# Remove rows with NaN values in the 'Reviews' column\\nmy_data = my_data[my_data['Reviews'].notnull()]\\n\\n# Create a TF-IDF vectorizer\\nmy_vectorizer = TfidfVectorizer()\\n\\n# Vectorize the text data\\nmy_X = my_vectorizer.fit_transform(my_data['Reviews'])\\n\\n# Perform clustering or other downstream tasks using my_X\", 'SKDBSCAN': <class 'sklearn.cluster._dbscan.DBSCAN'>, 'my_data':                                             Product Name Brand Name   Price  \\\n",
            "0      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "1      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "2      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "3      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "4      \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
            "...                                                  ...        ...     ...   \n",
            "12381  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12382  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12383  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12384  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "12385  Apple iPhone 4S 16GB 3G WiFi White Smartphone ...        NaN   74.95   \n",
            "\n",
            "       Rating                                            Reviews  Review Votes  \n",
            "0         5.0  I feel so LUCKY to have found this used (phone...           1.0  \n",
            "1         4.0  nice phone, nice up grade from my pantach revu...           0.0  \n",
            "2         5.0                                       Very pleased           0.0  \n",
            "3         4.0  It works good but it goes slow sometimes but i...           0.0  \n",
            "4         4.0  Great phone to replace my lost phone. The only...           0.0  \n",
            "...       ...                                                ...           ...  \n",
            "12381     4.0                                        Worked fine           0.0  \n",
            "12382     5.0  This phone works well. It replaced a damaged u...           0.0  \n",
            "12383     5.0  I am very happy with my iPhone 4s but you must...           0.0  \n",
            "12384     5.0                                            Perfect           0.0  \n",
            "12385     5.0                                      Perfect item.           0.0  \n",
            "\n",
            "[12385 rows x 6 columns], 'my_vectorizer': TfidfVectorizer(), 'my_X': <12385x11419 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 344866 stored elements in Compressed Sparse Row format>, '_i15': '# K-means clustering\\nprint(\"Performing K-means clustering...\")\\nmy_kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\\nmy_kmeans.fit(my_X)\\nprint(\"K-means clustering complete.\")\\n\\n# Plot K-means clusters\\nmy_tsne = TSNE(n_components=2, random_state=42, init=\\'random\\')\\nmy_X_tsne = my_tsne.fit_transform(my_X)\\nplt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_kmeans.labels_)\\nplt.title(\"K-means Clustering\")\\nplt.show()', 'my_kmeans': KMeans(n_clusters=5, random_state=42), 'my_tsne': TSNE(init='random', random_state=42), '_i16': \"\\n# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", 'train_data':                                                 reviews  sentiment\n",
            "0     a stirring , funny and finally transporting re...          1\n",
            "1     apparently reassembled from the cutting-room f...          0\n",
            "2     they presume their audience wo n't sit still f...          0\n",
            "3     this is a visually stunning rumination on love...          1\n",
            "4     jonathan parker 's bartleby should have been t...          1\n",
            "...                                                 ...        ...\n",
            "6915  painful , horrifying and oppressively tragic ,...          1\n",
            "6916  take care is nicely performed by a quintet of ...          0\n",
            "6917  the script covers huge , heavy topics in a bla...          0\n",
            "6918  a seriously bad film with seriously warped log...          0\n",
            "6919  a deliciously nonsensical comedy about a city ...          1\n",
            "\n",
            "[6920 rows x 2 columns], '_16':                                              reviews  sentiment\n",
            "0  a stirring , funny and finally transporting re...          1\n",
            "1  apparently reassembled from the cutting-room f...          0\n",
            "2  they presume their audience wo n't sit still f...          0\n",
            "3  this is a visually stunning rumination on love...          1\n",
            "4  jonathan parker 's bartleby should have been t...          1, '_i17': \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", 'test_data':                                                 reviews  sentiment\n",
            "0        no movement , no yuks , not much of anything .          0\n",
            "1     a gob of drivel so sickly sweet , even the eag...          0\n",
            "2     gangs of new york is an unapologetic mess , wh...          0\n",
            "3     we never really feel involved with the story ,...          0\n",
            "4               this is one of polanski 's best films .          1\n",
            "...                                                 ...        ...\n",
            "1816  an often-deadly boring , strange reading of a ...          0\n",
            "1817  the problem with concept films is that if the ...          0\n",
            "1818  safe conduct , however ambitious and well-inte...          0\n",
            "1819  a film made with as little wit , interest , an...          0\n",
            "1820  but here 's the real damn : it is n't funny , ...          0\n",
            "\n",
            "[1821 rows x 2 columns], '_17':                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1, '_i18': 'import nltk\\nimport re\\nimport string\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopword=nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nwl= WordNetLemmatizer()\\n\\ndef clean(reviews):\\n    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\\n    review = re.sub(\"^\\\\d+\\\\s|\\\\s\\\\d+\\\\s|\\\\s\\\\d+$\", \" \", reviews)\\n    tokens = re.split(\\'\\\\W+\\',reviews)\\n    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\\n    return review', 'nltk': <module 'nltk' from '/usr/local/lib/python3.10/dist-packages/nltk/__init__.py'>, 're': <module 're' from '/usr/lib/python3.10/re.py'>, 'string': <module 'string' from '/usr/lib/python3.10/string.py'>, 'stopword': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'WordNetLemmatizer': <class 'nltk.stem.wordnet.WordNetLemmatizer'>, 'wl': <WordNetLemmatizer>, 'clean': <function clean at 0x7957fb956050>, '_i19': 'import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Load and preprocess data\\ntrain_data = pd.read_csv(\\'stsa-train.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntrain_data[\\'sentiment\\'] = train_data[\\'reviews\\'].str[0].astype(int)\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str[2:]\\n\\ntest_data = pd.read_csv(\\'stsa-test.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntest_data[\\'sentiment\\'] = test_data[\\'reviews\\'].str[0].astype(int)\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str[2:]\\n\\n# Define custom text preprocessing function\\nimport nltk\\nimport re\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopwords = nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    text = re.sub(r\\'[^a-zA-Z\\\\s]\\', \\'\\', text)  # Remove non-alphabetic characters\\n    tokens = re.split(r\\'\\\\W+\\', text.lower())\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\\n    return \\' \\'.join(tokens)\\n\\n# Create TF-IDF vectorizer with custom preprocessing\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', preprocessor=preprocess_text, stop_words=\\'english\\', max_features=13343)\\n\\n# Fit and transform data\\nX_tfidf = tfidf_vect.fit_transform(train_data[\\'reviews\\'])\\nX_test_tfidf = tfidf_vect.transform(test_data[\\'reviews\\'])\\n\\nprint(f\"Training data shape: {X_tfidf.shape}\")\\nprint(f\"Test data shape: {X_test_tfidf.shape}\")', 'stopwords': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'lemmatizer': <WordNetLemmatizer>, 'preprocess_text': <function preprocess_text at 0x7957de64f6d0>, 'tfidf_vect': TfidfVectorizer(max_features=13343,\n",
            "                preprocessor=<function preprocess_text at 0x7957de64f6d0>,\n",
            "                stop_words='english'), 'X_tfidf': <6920x13081 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 58718 stored elements in Compressed Sparse Row format>, 'X_test_tfidf': <1821x13081 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 13548 stored elements in Compressed Sparse Row format>, '_i20': \"import pandas as pd\\nfrom sklearn import datasets, linear_model\\nfrom sklearn.model_selection import train_test_split\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.naive_bayes import MultinomialNB\\nvectorizer = TfidfVectorizer()\\nX_tfidf_df = vectorizer.fit_transform(train_data['reviews'].values)\\nprint(globals())\\nmnb = MultinomialNB()\\n\\n# Sampling the training set\\nx_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\\n                                                test_size=0.2, random_state=42)\\n# Model fit using .fit()\\nmodel_mnb = mnb.fit(x_train,y_train)\\ny_pred_mnb = model_mnb.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\\nprint(classification_report(y_test,y_pred_mnb))\", 'datasets': <module 'sklearn.datasets' from '/usr/local/lib/python3.10/dist-packages/sklearn/datasets/__init__.py'>, 'linear_model': <module 'sklearn.linear_model' from '/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/__init__.py'>, 'train_test_split': <function train_test_split at 0x7958d3bd63b0>, 'accuracy_score': <function accuracy_score at 0x7958d40ed480>, 'classification_report': <function classification_report at 0x7958d40edd80>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'X_tfidf_df': <6920x13789 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 106001 stored elements in Compressed Sparse Row format>, 'mnb': MultinomialNB(), 'x_train': <5536x13789 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 84709 stored elements in Compressed Sparse Row format>, 'x_test': <1384x13789 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 21292 stored elements in Compressed Sparse Row format>, 'y_train': array([1, 0, 1, ..., 0, 0, 1]), 'y_test': array([0, 1, 1, ..., 1, 0, 1]), 'model_mnb': MultinomialNB(), 'y_pred_mnb': array([0, 1, 1, ..., 1, 0, 1]), '_i21': 'from sklearn.model_selection import cross_val_score\\nscores = cross_val_score(mnb, x_test, y_test, cv=10)\\nprint(\"MultinominalNB score: \",scores.mean())', 'cross_val_score': <function cross_val_score at 0x7958d3bd6b90>, 'scores': array([0.74100719, 0.76258993, 0.76258993, 0.69064748, 0.71014493,\n",
            "       0.7826087 , 0.8115942 , 0.74637681, 0.73913043, 0.65942029]), '_i22': \"from sklearn.svm import LinearSVC\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\\nfrom xgboost import XGBClassifier\\n\\nsvm = LinearSVC()\\nmodel_svm = svm.fit(x_train,y_train)\\ny_pred_svm = model_svm.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_svm,y_test))\\nprint(classification_report(y_test,y_pred_svm))\", 'LinearSVC': <class 'sklearn.svm._classes.LinearSVC'>, 'DecisionTreeClassifier': <class 'sklearn.tree._classes.DecisionTreeClassifier'>, 'RandomForestClassifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>, 'cross_validate': <function cross_validate at 0x7958d3bd6950>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, 'XGBClassifier': <class 'xgboost.sklearn.XGBClassifier'>, 'svm': LinearSVC(), 'model_svm': LinearSVC(), 'y_pred_svm': array([0, 1, 1, ..., 1, 0, 1]), '_i23': 'from sklearn.model_selection import cross_val_score\\nscores = cross_val_score(svm, x_test, y_test, cv=10)\\nprint(\"SVM score:\",scores.mean())', '_i24': \"# 3) KNeighbor\\nfrom sklearn.neighbors import KNeighborsClassifier\\n\\nknn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\\nmodel_knn = knn.fit(x_train,y_train)\\ny_pred_knn = model_knn.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_knn,y_test))\\nprint(classification_report(y_test,y_pred_knn))\", 'KNeighborsClassifier': <class 'sklearn.neighbors._classification.KNeighborsClassifier'>, 'knn': KNeighborsClassifier(n_jobs=-1), 'model_knn': KNeighborsClassifier(n_jobs=-1), 'y_pred_knn': array([0, 1, 1, ..., 1, 0, 1]), '_i25': \"\\n# Saving the train data in a dataframe\\nimport pandas as pd\\n\\ntrain_data = pd.read_csv('stsa-train.txt', sep='delimiter=', header=None, names=['reviews'])\\ntrain_data['sentiment'] = train_data['reviews'].str[0].astype(int)\\ntrain_data['reviews'] = train_data['reviews'].str[2:]\\ntrain_data.head()\", '_25':                                              reviews  sentiment\n",
            "0  a stirring , funny and finally transporting re...          1\n",
            "1  apparently reassembled from the cutting-room f...          0\n",
            "2  they presume their audience wo n't sit still f...          0\n",
            "3  this is a visually stunning rumination on love...          1\n",
            "4  jonathan parker 's bartleby should have been t...          1, '_i26': \"#write your code here\\nimport pandas as pd\\n\\ntest_data = pd.read_csv('stsa-test.txt', sep='delimiter=', header=None, names=['reviews'])\\ntest_data['sentiment'] = test_data['reviews'].str[0].astype(int)\\ntest_data['reviews'] = test_data['reviews'].str[2:]\\ntest_data.head()\", '_26':                                              reviews  sentiment\n",
            "0     no movement , no yuks , not much of anything .          0\n",
            "1  a gob of drivel so sickly sweet , even the eag...          0\n",
            "2  gangs of new york is an unapologetic mess , wh...          0\n",
            "3  we never really feel involved with the story ,...          0\n",
            "4            this is one of polanski 's best films .          1, '_i27': 'import nltk\\nimport re\\nimport string\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopword=nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nwl= WordNetLemmatizer()\\n\\ndef clean(reviews):\\n    review =\"\".join([word.lower() for word in reviews if word not in string.punctuation])\\n    review = re.sub(\"^\\\\d+\\\\s|\\\\s\\\\d+\\\\s|\\\\s\\\\d+$\", \" \", reviews)\\n    tokens = re.split(\\'\\\\W+\\',reviews)\\n    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\\n    return review', '_i28': 'import pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Load and preprocess data\\ntrain_data = pd.read_csv(\\'stsa-train.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntrain_data[\\'sentiment\\'] = train_data[\\'reviews\\'].str[0].astype(int)\\ntrain_data[\\'reviews\\'] = train_data[\\'reviews\\'].str[2:]\\n\\ntest_data = pd.read_csv(\\'stsa-test.txt\\', sep=\\'delimiter=\\', header=None, names=[\\'reviews\\'])\\ntest_data[\\'sentiment\\'] = test_data[\\'reviews\\'].str[0].astype(int)\\ntest_data[\\'reviews\\'] = test_data[\\'reviews\\'].str[2:]\\n\\n# Define custom text preprocessing function\\nimport nltk\\nimport re\\nnltk.download(\\'stopwords\\')\\nnltk.download(\\'wordnet\\')\\nstopwords = nltk.corpus.stopwords.words(\\'english\\')\\nfrom nltk.stem import WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\ndef preprocess_text(text):\\n    text = re.sub(r\\'[^a-zA-Z\\\\s]\\', \\'\\', text)  # Remove non-alphabetic characters\\n    tokens = re.split(r\\'\\\\W+\\', text.lower())\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords]\\n    return \\' \\'.join(tokens)\\n\\n# Create TF-IDF vectorizer with custom preprocessing\\ntfidf_vect = TfidfVectorizer(analyzer=\\'word\\', preprocessor=preprocess_text, stop_words=\\'english\\', max_features=13343)\\n\\n# Fit and transform data\\nX_tfidf = tfidf_vect.fit_transform(train_data[\\'reviews\\'])\\nX_test_tfidf = tfidf_vect.transform(test_data[\\'reviews\\'])\\n\\nprint(f\"Training data shape: {X_tfidf.shape}\")\\nprint(f\"Test data shape: {X_test_tfidf.shape}\")', '_i29': \"import pandas as pd\\nfrom sklearn import datasets, linear_model\\nfrom sklearn.model_selection import train_test_split\\nfrom matplotlib import pyplot as plt\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.naive_bayes import MultinomialNB\\nvectorizer = TfidfVectorizer()\\nX_tfidf_df = vectorizer.fit_transform(train_data['reviews'].values)\\nprint(globals())\\nmnb = MultinomialNB()\\n\\n# Sampling the training set\\nx_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\\n                                                test_size=0.2, random_state=42)\\n# Model fit using .fit()\\nmodel_mnb = mnb.fit(x_train,y_train)\\ny_pred_mnb = model_mnb.predict(x_test)\\nprint('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\\nprint(classification_report(y_test,y_pred_mnb))\"}\n",
            "Accuracy 0.7969653179190751\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.70      0.77       671\n",
            "           1       0.76      0.89      0.82       713\n",
            "\n",
            "    accuracy                           0.80      1384\n",
            "   macro avg       0.81      0.79      0.79      1384\n",
            "weighted avg       0.81      0.80      0.79      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(mnb, x_test, y_test, cv=10)\n",
        "print(\"MultinominalNB score: \",scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4i-OSwStL3F",
        "outputId": "7803d99a-df74-4bc0-b822-21c8149d8900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinominalNB score:  0.7434365551037432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "svm = LinearSVC()\n",
        "model_svm = svm.fit(x_train,y_train)\n",
        "y_pred_svm = model_svm.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_svm,y_test))\n",
        "print(classification_report(y_test,y_pred_svm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8grJo2btNem",
        "outputId": "c5288030-674c-46f2-8255-e6217b283142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.809971098265896\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.76      0.80       671\n",
            "           1       0.79      0.85      0.82       713\n",
            "\n",
            "    accuracy                           0.81      1384\n",
            "   macro avg       0.81      0.81      0.81      1384\n",
            "weighted avg       0.81      0.81      0.81      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(svm, x_test, y_test, cv=10)\n",
        "print(\"SVM score:\",scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPhtHfdltQg4",
        "outputId": "2dad9c71-6484-491f-b99c-dd7e2fcad107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM score: 0.740610989469294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) KNeighbor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n",
        "model_knn = knn.fit(x_train,y_train)\n",
        "y_pred_knn = model_knn.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_knn,y_test))\n",
        "print(classification_report(y_test,y_pred_knn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPIPCAb0tRIg",
        "outputId": "5560833e-1a14-43a0-f9ae-5da19ccef8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.7290462427745664\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.67      0.70       671\n",
            "           1       0.72      0.79      0.75       713\n",
            "\n",
            "    accuracy                           0.73      1384\n",
            "   macro avg       0.73      0.73      0.73      1384\n",
            "weighted avg       0.73      0.73      0.73      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating cross-value score\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(knn, x_test, y_test, cv=10)\n",
        "print(\"KNN score:\",scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8yfJtEitbtb",
        "outputId": "09863c3e-2d58-4e8a-98e4-1f62e18ca860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN score: 0.6993639870712126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "model_dt = dt.fit(x_train,y_train)\n",
        "y_pred_dt = model_dt.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_dt,y_test))\n",
        "print(classification_report(y_test,y_pred_dt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QoDmVmGteS0",
        "outputId": "efaeb078-eb20-4f46-c1bf-dff74b0dc1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.5960982658959537\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.55      0.57       671\n",
            "           1       0.60      0.64      0.62       713\n",
            "\n",
            "    accuracy                           0.60      1384\n",
            "   macro avg       0.60      0.59      0.59      1384\n",
            "weighted avg       0.60      0.60      0.60      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating cross value score\n",
        "scores = cross_val_score(dt, x_test, y_test, cv=10)\n",
        "print(\"Decision tree score:\",scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwu3F9FYtccn",
        "outputId": "ec5b28a0-4ac6-48d1-ce0f-40cddd6207bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision tree score: 0.5888854134084038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "model_rf = rf.fit(x_train,y_train)\n",
        "y_pred_rf = model_rf.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_rf,y_test))\n",
        "print(classification_report(y_test,y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW8zmd8otjGh",
        "outputId": "2b94871d-f8cc-4504-c876-996b2131a960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.7333815028901735\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.66      0.71       671\n",
            "           1       0.71      0.81      0.76       713\n",
            "\n",
            "    accuracy                           0.73      1384\n",
            "   macro avg       0.74      0.73      0.73      1384\n",
            "weighted avg       0.74      0.73      0.73      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest - Calculate cross value score\n",
        "cores = cross_val_score(rf, x_test, y_test, cv=10)\n",
        "print(\"Random forest score\",scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwAi_euttlYd",
        "outputId": "a74462b7-680c-4ca6-f667-20927e0e8b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random forest score 0.5888854134084038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier()\n",
        "model_xgb = xgb.fit(x_train,y_train)\n",
        "y_pred_xgb = model_xgb.predict(x_test)\n",
        "print('Accuracy %s' % accuracy_score(y_pred_xgb,y_test))\n",
        "print(classification_report(y_test,y_pred_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k8mKQSctm4Y",
        "outputId": "87380a16-dedc-42bb-ade3-a9275c01a02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.713150289017341\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.61      0.67       671\n",
            "           1       0.69      0.81      0.75       713\n",
            "\n",
            "    accuracy                           0.71      1384\n",
            "   macro avg       0.72      0.71      0.71      1384\n",
            "weighted avg       0.72      0.71      0.71      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost - Calculate cross value score\n",
        "scores = cross_val_score(xgb, x_test, y_test, cv=10)\n",
        "print(\"XGBoost score:\",scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIOWifzctpCZ",
        "outputId": "6de651fa-ea2f-46ef-ec02-b3019088160b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost score: 0.6589458867688458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-Rating-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c6ce79-3b3e-4441-fc43-2448bcbaf9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-b59d9d4e6b23>:14: DtypeWarning: Columns (0,1,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  my_data = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "import gensim\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN as SKDBSCAN\n",
        "\n",
        "# Install the sentence_transformers module if it is not already installed\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Load the dataset\n",
        "my_data = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
        "\n",
        "# Remove rows with NaN values in the 'Reviews' column\n",
        "my_data = my_data[my_data['Reviews'].notnull()]\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "my_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Vectorize the text data\n",
        "my_X = my_vectorizer.fit_transform(my_data['Reviews'])\n",
        "\n",
        "# Perform clustering or other downstream tasks using my_X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-means clustering\n",
        "print(\"Performing K-means clustering...\")\n",
        "my_kmeans = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\n",
        "my_kmeans.fit(my_X)\n",
        "print(\"K-means clustering complete.\")\n",
        "\n",
        "# Plot K-means clusters\n",
        "my_tsne = TSNE(n_components=2, random_state=42, init='random')\n",
        "my_X_tsne = my_tsne.fit_transform(my_X)\n",
        "plt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_kmeans.labels_)\n",
        "plt.title(\"K-means Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wd2lMLOqiyx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45a1fc1-c758-45ff-be4c-092e4f1e16ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing K-means clustering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-means clustering complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DBSCAN clustering\n",
        "print(\"Performing DBSCAN clustering...\")\n",
        "my_dbscan = SKDBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed\n",
        "my_dbscan.fit(my_X)\n",
        "print(\"DBSCAN clustering complete.\")\n",
        "\n",
        "# Plot DBSCAN clusters\n",
        "my_tsne = TSNE(n_components=2, random_state=42, init='random')\n",
        "my_X_tsne = my_tsne.fit_transform(my_X)\n",
        "plt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_dbscan.labels_)\n",
        "plt.title(\"DBSCAN Clustering\")"
      ],
      "metadata": {
        "id": "-iSpgt5LVKI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hierarchical clustering\n",
        "print(\"Performing hierarchical clustering...\")\n",
        "my_hierarchical = AgglomerativeClustering(n_clusters=5) # Adjust the number of clusters as needed\n",
        "my_hierarchical.fit(my_X.toarray())\n",
        "print(\"Hierarchical clustering complete.\")\n",
        "\n",
        "# Plot hierarchical clusters\n",
        "my_tsne = TSNE(n_components=2, random_state=42, init='random')\n",
        "my_X_tsne = my_tsne.fit_transform(my_X)\n",
        "plt.scatter(my_X_tsne[:, 0], my_X_tsne[:, 1], c=my_hierarchical.labels_)\n",
        "plt.title(\"Hierarchical Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BS02nA6jaMOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec clustering\n",
        "print(\"Performing Word2Vec clustering...\")\n",
        "my_texts = [text.split() for text in my_data['Reviews']]\n",
        "my_model = gensim.models.Word2Vec(my_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "my_word_vectors = [sum(my_model.wv[word] for word in text) / len(text) for text in my_texts]\n",
        "my_kmeans_word2vec = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\n",
        "my_kmeans_word2vec.fit(my_word_vectors)\n",
        "print(\"Word2Vec clustering complete.\")\n",
        "\n",
        "my_word_vectors = np.array(my_word_vectors)\n",
        "my_tsne = TSNE(n_components=2, random_state=42, init='random')\n",
        "my_word_vectors_tsne = my_tsne.fit_transform(my_word_vectors)\n",
        "plt.scatter(my_word_vectors_tsne[:, 0], my_word_vectors_tsne[:, 1], c=my_kmeans_word2vec.labels_)\n",
        "plt.title(\"Word2Vec Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "17BolGy3aQxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec clustering\n",
        "print(\"Performing Word2Vec clustering...\")\n",
        "my_texts = [text.split() for text in my_data['Reviews']]\n",
        "my_model = gensim.models.Word2Vec(my_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "my_word_vectors = [sum(my_model.wv[word] for word in text) / len(text) for text in my_texts]\n",
        "my_kmeans_word2vec = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\n",
        "my_kmeans_word2vec.fit(my_word_vectors)\n",
        "print(\"Word2Vec clustering complete.\")\n",
        "\n",
        "my_word_vectors = np.array(my_word_vectors)\n",
        "my_tsne = TSNE(n_components=2, random_state=42, init='random')\n",
        "my_word_vectors_tsne = my_tsne.fit_transform(my_word_vectors)\n",
        "plt.scatter(my_word_vectors_tsne[:, 0], my_word_vectors_tsne[:, 1], c=my_kmeans_word2vec.labels_)\n",
        "plt.title(\"Word2Vec Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XAz7LACGaVJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT clustering\n",
        "print(\"Performing BERT clustering...\")\n",
        "my_bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "my_embeddings = my_bert_model.encode(my_data['Reviews'].tolist())\n",
        "my_kmeans_bert = KMeans(n_clusters=5, random_state=42)  # Adjust the number of clusters as needed\n",
        "my_kmeans_bert.fit(my_embeddings)\n",
        "print(\"BERT clustering complete.\")\n",
        "\n",
        "my_tsne = TSNE(n_components=2, random_state=42, init='random')\n",
        "my_embeddings_tsne = my_tsne.fit_transform(my_embeddings)\n",
        "plt.scatter(my_embeddings_tsne[:, 0], my_embeddings_tsne[:, 1], c=my_kmeans_bert.labels_)\n",
        "plt.title(\"BERT Clustering\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v4X52CyKaX4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEs-OoDEhTW4"
      },
      "source": [
        "# Mandatory Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKC7suYhVl0"
      },
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "Unsupervised clustering algorithms, K-means, DBSCAN, and hierarchical clustering algorithms are most commonly used for\n",
        "grouping similar data points depending on their features. The k-means algorithm is based on the centroid, here each data\n",
        "point is placed in such a way that it is nearest to the centroid to minimize the cluster variance. DBSCAN algorithm is based\n",
        "on density, here points placed closely are considered as a group, and points that are isolated are treated as noise.\n",
        "Hierarchical clustering uses either a top-down or bottom-up approach that merges or splits data points depending on\n",
        "distance or similarity.\n",
        "Word2Vec and BERT are language models that are used for natural language processing(NLP) tasks such as machine translation,\n",
        "text classification, and sentiment analysis. Word2Vec uses a neural network approach, here in this approach the algorithm\n",
        "learns vector representations of words in a corpus by predicting the context words given a target word. BERT uses a\n",
        "transformer- based approach, here it generates contextualized word embeddings using a technique called bidirectional encoding.\n",
        "The outputs of these algorithms depend on the problem and the nature of the data that is being analyzed. When the number of\n",
        "clusters is known before, we can use K-Means and Hierarchical clustering algorithms. When the given data contains outliers or\n",
        "has varying densities DBSCAN is useful. Word2Vec and BERT are language models that can learn complex semantic relationships\n",
        "between words and are often used for natural language processing tasks. However, depending on the problem and the\n",
        "characteristics of the data that is given to be analyzed helps us decide which algorithm to use."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}